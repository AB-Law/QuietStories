# LLM Provider Configuration
MODEL_PROVIDER=openai  # Options: openai, ollama, generic
OPENAI_API_BASE=https://api.openai.com/v1
OPENAI_API_KEY=your_openai_api_key_here
MODEL_NAME=gpt-4

# For Ollama (local)
# MODEL_PROVIDER=ollama
# OPENAI_API_BASE=http://localhost:11434/v1
# OPENAI_API_KEY=dummy_key
# MODEL_NAME=llama2

# For Generic HTTP endpoint
# MODEL_PROVIDER=generic
# OPENAI_API_BASE=http://your-server:8000/v1
# OPENAI_API_KEY=your_api_key
# MODEL_NAME=your-model

# Server Configuration
HOST=0.0.0.0
PORT=8000
DEBUG=false

# Monte Carlo Configuration
MONTE_CARLO_TURNS=100
NEGATIVITY_MIN_FAIL_RATE=0.25
