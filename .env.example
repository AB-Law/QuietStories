# QuietStories Environment Configuration
# Copy this file to .env and update with your settings

# ============================================================================
# LLM Provider Configuration
# ============================================================================

# Choose your LLM provider: openai, lmstudio, ollama
MODEL_PROVIDER=openai

# Model name to use
MODEL_NAME=gpt-4o-mini

# ============================================================================
# OpenAI Configuration (if MODEL_PROVIDER=openai)
# ============================================================================

# Your OpenAI API key (required for OpenAI provider)
OPENAI_API_KEY=your_api_key_here

# ============================================================================
# LM Studio Configuration (if MODEL_PROVIDER=lmstudio)
# ============================================================================

# LM Studio API base URL (default: http://localhost:1234/v1)
# For Docker: use http://host.docker.internal:1234/v1
LMSTUDIO_API_BASE=http://localhost:1234/v1

# ============================================================================
# Ollama Configuration (if MODEL_PROVIDER=ollama)
# ============================================================================

# Ollama API base URL (default: http://localhost:11434/v1)
# For Docker: use http://host.docker.internal:11434/v1
OPENAI_API_BASE=http://localhost:11434/v1

# ============================================================================
# Embedding Configuration
# ============================================================================

# Embedding provider: openai, ollama, lmstudio, none
EMBEDDING_PROVIDER=openai

# Embedding model name
EMBEDDING_MODEL_NAME=text-embedding-3-small

# Embedding API base (for local embeddings)
# For LM Studio: http://localhost:1234/v1
# For Ollama: http://localhost:11434/v1
# For Docker: use host.docker.internal
EMBEDDING_API_BASE=http://localhost:1234/v1

# ============================================================================
# Database Configuration
# ============================================================================

# Path to SQLite database file
DATABASE_PATH=/app/data/quietstories.db

# ============================================================================
# Logging Configuration
# ============================================================================

# Log level: DEBUG, VERBOSE, INFO, WARNING, ERROR, CRITICAL
# - DEBUG: Detailed diagnostic information (includes all internal state)
# - VERBOSE: Enhanced logging showing full LLM requests/responses
# - INFO: General application flow (default for production)
# - WARNING: Potential issues and warnings
# - ERROR: Error messages only
# - CRITICAL: Critical failures only
LOG_LEVEL=INFO

# Enable console logs (true/false)
# Set to false for production (file-only logs)
ENABLE_CONSOLE_LOGS=false

# Log file path (relative to project root)
LOG_FILE=logs/app.log

# ============================================================================
# Frontend Configuration
# ============================================================================

# API URL for frontend (browser-accessible URL)
# For local development: http://localhost:8000
# For production: https://api.yourdomain.com
VITE_API_URL=http://localhost:8000

# ============================================================================
# Server Configuration
# ============================================================================

# Host to bind to (default: 0.0.0.0 for all interfaces)
HOST=0.0.0.0

# Port to listen on (default: 8000)
PORT=8000

# Number of worker processes (for production)
WORKERS=1

# ============================================================================
# Production Deployment Notes
# ============================================================================

# For production deployment with docker-compose.prod.yml:
# 1. Copy this file to .env
# 2. Update API keys and provider settings
# 3. Set VITE_API_URL to your production domain
# 4. Set ENABLE_CONSOLE_LOGS=false
# 5. Set LOG_LEVEL=INFO or WARNING
# 6. Configure WORKERS based on your server capacity

# For local development:
# 1. Copy this file to .env
# 2. Set MODEL_PROVIDER to lmstudio or ollama
# 3. Update LMSTUDIO_API_BASE or OPENAI_API_BASE
# 4. Set ENABLE_CONSOLE_LOGS=true
# 5. Set LOG_LEVEL=DEBUG for detailed logging

# For Docker deployment with local LLMs:
# - Use host.docker.internal instead of localhost
# - Example: LMSTUDIO_API_BASE=http://host.docker.internal:1234/v1
